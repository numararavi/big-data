{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on pertemuan 1 Pengenalan Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalasi anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda adalah platform distribusi Python yang menyertakan berbagai alat pengembangan, termasuk Jupyter Notebook. Ikuti langkah-langkah instalasi sesuai sistem operasi:\n",
    "  - Unduh Anaconda: Download anaconda\n",
    "  - Instal sesuai instruksi yang ada di situs web tersebut (Windows/Mac/Linux).\n",
    "\n",
    "- **Langkah 2: Menginstal PySpark di Anaconda**\n",
    "  Setelah Anaconda terinstal, tambahkan PySpark:\n",
    "  ```bash\n",
    "  Pada Command prompt ketik pip install pyspark==3.4.1 \n",
    "  ```\n",
    "\n",
    "- **Langkah 3: Menginstal Pandas**\n",
    "  Untuk memudahkan data analysis, install Pandas:\n",
    "  ```bash\n",
    "  Pada Command prompt ketik pip install pandas\n",
    "  ```\n",
    "\n",
    "- **Langkah 4: Menginstal Findspark**\n",
    "  ```bash\n",
    "  Pada Command Prompt ketik pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kode tersebut menggunakan import untuk memanggil modul findspark, modul tersebut memudahkan untuk integrasi apache spark dengan python. Fungsi init() dari modul findspark digunakan untuk menambahkan direktori Spark ke dalam sys.path di Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Nama|Usia|\n",
      "+-----+----+\n",
      "|  Ali|  34|\n",
      "| Budi|  23|\n",
      "|Citra|  29|\n",
      "| Dina|  45|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Memulai Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataPractice\").getOrCreate()\n",
    "\n",
    "# Membuat DataFrame sederhana\n",
    "data = [(\"Ali\", 34), (\"Budi\", 23), (\"Citra\", 29), (\"Dina\", 45)]\n",
    "columns = [\"Nama\", \"Usia\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Menampilkan DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi mengimpor class sparkSession dari modul pyspark. Lalu membuat objek spark sesision dengan nama big data practice. Lalu membuat list tupple yang akan menjadi dataframe.\n",
    "Lalu membuat list yang berisi nama nama kolom, Lalu menggunakan dfshow untuk menampilkan isi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tugas 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-------+---------+\n",
      "| Nama|Usia|Pekerjaan|   Hobi|   Gender|\n",
      "+-----+----+---------+-------+---------+\n",
      "|  Ali|  34|   petani|Mancing|Laki Laki|\n",
      "| Budi|  23|      PNS|  Tidur|laki laki|\n",
      "|Citra|  29|developer|  scrol|perempuan|\n",
      "| Dina|  45| pedagang|ngeslot|perempuan|\n",
      "+-----+----+---------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Memulai Spark session\n",
    "spark = SparkSession.builder.appName(\"BigData\").getOrCreate()\n",
    "\n",
    "# Membuat DataFrame sederhana\n",
    "data = [(\"Ali\", 34,\"petani\",\"Mancing\",\"Laki Laki\"), (\"Budi\", 23,\"PNS\",\"Tidur\",\"laki laki\"), (\"Citra\", 29,\"developer\",\"scrol\",\"perempuan\"), (\"Dina\", 45,\"pedagang\",\"ngeslot\",\"perempuan\")]\n",
    "columns = [\"Nama\", \"Usia\",\"Pekerjaan\",\"Hobi\",\"Gender\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Menampilkan DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi mengimpor class sparkSession dari modul pyspark. Lalu membuat objek spark sesision dengan nama big data practice. Lalu membuat list tupple yang akan menjadi dataframe.\n",
    "Lalu membuat list yang berisi nama nama kolom, Lalu menggunakan dfshow untuk menampilkan isi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktik pyspark lanjutan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Nama|Usia|\n",
      "+-----+----+\n",
      "|  Ali|  34|\n",
      "| Budi|  23|\n",
      "|Citra|  29|\n",
      "| Dina|  45|\n",
      "+-----+----+\n",
      "\n",
      "+----+----+\n",
      "|Nama|Usia|\n",
      "+----+----+\n",
      "| Ali|  34|\n",
      "|Dina|  45|\n",
      "+----+----+\n",
      "\n",
      "+---------+\n",
      "|avg(Usia)|\n",
      "+---------+\n",
      "|    32.75|\n",
      "+---------+\n",
      "\n",
      "+-----+----+\n",
      "| Nama|Usia|\n",
      "+-----+----+\n",
      "| Dina|  45|\n",
      "|  Ali|  34|\n",
      "|Citra|  29|\n",
      "| Budi|  23|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Memulai Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataPractice\").getOrCreate()\n",
    "\n",
    "# Membuat DataFrame sederhana\n",
    "data = [(\"Ali\", 34), (\"Budi\", 23), (\"Citra\", 29), (\"Dina\", 45)]\n",
    "columns = [\"Nama\", \"Usia\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Menampilkan DataFrame\n",
    "df.show()\n",
    "\n",
    "# Filtering data\n",
    "df_filtered = df.filter(df['Usia'] > 30)\n",
    "df_filtered.show()\n",
    "\n",
    "# Menghitung rata-rata usia\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy().agg(avg(\"Usia\")).show()\n",
    "\n",
    "# Mengurutkan data berdasarkan usia\n",
    "df_sorted = df.orderBy(\"Usia\", ascending=False)\n",
    "df_sorted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menginisialisasi spark session yaitu mengimpor kelas sparksesson, from pyspark.sql import SparkSession: Mengimpor kelas SparkSession untuk membuat lingkungan Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan filter, penghitungan rata-rata, dan pengurutan data menggunakan PySpark, data = [(\"Ali\", 34), (\"Budi\", 23), (\"Citra\", 29), (\"Dina\", 45)]: Membuat data dalam bentuk list of tuples, di mana setiap tuple mewakili satu baris data (nama dan usia).\n",
    "columns = [\"Nama\", \"Usia\"]: Mendefinisikan nama-nama kolom untuk DataFrame.\n",
    "df = spark.createDataFrame(data, columns): Membuat DataFrame dari data dan nama kolom. df.show(): Menampilkan semua isi DataFrame.\n",
    "df_filtered = df.filter(df['Usia'] > 30): Memfilter DataFrame untuk mendapatkan baris-baris dengan usia lebih dari 30.\n",
    "df.groupBy().agg(avg(\"Usia\")): Mengelompokkan semua data dan menghitung rata-rata usia.\n",
    "df_sorted = df.orderBy(\"Usia\", ascending=False): Mengurutkan DataFrame berdasarkan kolom \"Usia\" secara descending (dari yang terbesar ke terkecil)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Nama|Usia|\n",
      "+------+----+\n",
      "|   Ali|  23|\n",
      "|supini|  11|\n",
      "|  Diva|  14|\n",
      "|Amanda|  20|\n",
      "+------+----+\n",
      "\n",
      "+----+----+\n",
      "|Nama|Usia|\n",
      "+----+----+\n",
      "+----+----+\n",
      "\n",
      "+---------+\n",
      "|avg(Usia)|\n",
      "+---------+\n",
      "|     17.0|\n",
      "+---------+\n",
      "\n",
      "+------+----+\n",
      "|  Nama|Usia|\n",
      "+------+----+\n",
      "|   Ali|  23|\n",
      "|Amanda|  20|\n",
      "|  Diva|  14|\n",
      "|supini|  11|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Memulai Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataPractice\").getOrCreate()\n",
    "\n",
    "# Membuat DataFrame sederhana\n",
    "data = [(\"Ali\", 23), (\"supini\", 11), (\"Diva\", 14), (\"Amanda\", 20)]\n",
    "columns = [\"Nama\", \"Usia\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Menampilkan DataFrame\n",
    "df.show()\n",
    "\n",
    "# Filtering data\n",
    "df_filtered = df.filter(df['Usia'] > 30)\n",
    "df_filtered.show()\n",
    "\n",
    "# Menghitung rata-rata usia\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy().agg(avg(\"Usia\")).show()\n",
    "\n",
    "# Mengurutkan data berdasarkan usia\n",
    "df_sorted = df.orderBy(\"Usia\", ascending=False)\n",
    "df_sorted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.show(): Menampilkan semua isi DataFrame.\n",
    "df_filtered = df.filter(df['Usia'] > 30): Memfilter DataFrame untuk mendapatkan baris-baris dengan usia lebih dari 30.\n",
    "df.groupBy().agg(avg(\"Usia\")): Mengelompokkan semua data dan menghitung rata-rata usia.\n",
    "df_sorted = df.orderBy(\"Usia\", ascending=False): Mengurutkan DataFrame berdasarkan kolom \"Usia\" secara descending (dari yang terbesar ke terkecil)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggunakan pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama</th>\n",
       "      <th>Usia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Budi</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Citra</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dina</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Nama  Usia\n",
       "0    Ali    34\n",
       "1   Budi    23\n",
       "2  Citra    29\n",
       "3   Dina    45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membuat DataFrame Pandas\n",
    "data_pandas = {\"Nama\": [\"Ali\", \"Budi\", \"Citra\", \"Dina\"], \"Usia\": [34, 23, 29, 45]}\n",
    "df_pandas = pd.DataFrame(data_pandas)\n",
    "\n",
    "# Menampilkan DataFrame Pandas\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan pertama menginport modul, lalu membantu buat frame data dengan mengisi data dengan nama dan usia, \n",
    "Lalu membuat data frame pandas dan setelah selesai tampilkan menggunakan df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 3**: Modifikasi DataFrame Pandas dengan menambahkan kolom baru dan melakukan operasi seperti filtering data berdasarkan usia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Nama  usia     Hobi\n",
      "3  Yudhistira    35  belajar\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#membuat data frame pandas\n",
    "data_pandas={\"Nama\":[\"Ariel\",\"Numara\",\"Ravi\",\"Yudhistira\"],\"usia\":[21,22,21,35],\"Hobi\":[\"tidur\",\"ngegame\",\"ngaji\",\"belajar\"]}\n",
    "df_pandas=pd.DataFrame(data_pandas)\n",
    "data_dibawah_22=df_pandas[df_pandas[\"usia\"]>22]\n",
    "#menampilkan data frame pandas\n",
    "print(data_dibawah_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "menginport modul pandas, lalu membuat dataframe sesuai data yang disesuaikan, lalu filter data menggunakan var data dibawah 22, Lalu menampilkan data menggunakan print(data dibawah 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Nama  Usia Pekerjaan\n",
      "0    Ali    34    Dokter\n",
      "1   Budi    23      Guru\n",
      "2  Citra    29  Insinyur\n",
      "3   Dina    45   Perawat\n",
      "            Usia\n",
      "count   4.000000\n",
      "mean   32.750000\n",
      "std     9.322911\n",
      "min    23.000000\n",
      "25%    27.500000\n",
      "50%    31.500000\n",
      "75%    36.750000\n",
      "max    45.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membuat DataFrame Pandas\n",
    "data_pandas = {\"Nama\": [\"Ali\", \"Budi\", \"Citra\", \"Dina\"], \"Usia\": [34, 23, 29, 45]}\n",
    "df_pandas = pd.DataFrame(data_pandas)\n",
    "\n",
    "# Membuat DataFrame kedua\n",
    "data_pandas_2 = {\"Nama\": [\"Ali\", \"Budi\", \"Citra\", \"Dina\"], \"Pekerjaan\": [\"Dokter\", \"Guru\", \"Insinyur\", \"Perawat\"]}\n",
    "df_pandas_2 = pd.DataFrame(data_pandas_2)\n",
    "\n",
    "# Join antara dua DataFrame\n",
    "df_joined = pd.merge(df_pandas, df_pandas_2, on=\"Nama\")\n",
    "print(df_joined)\n",
    "\n",
    "# Menghitung statistik deskriptif\n",
    "print(df_pandas.describe())\n",
    "\n",
    "# Plotting Data\n",
    "import matplotlib.pyplot as plt\n",
    "df_pandas['Usia'].plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama menggunakan import untuk memanggil modul pandas, Lalu buat beberapa dalam dua data frame. lalu Gabungkan kedua nya menggunakan df_pandas. diskrif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 4**: Lakukan penggabungan DataFrame dan visualisasikan data dengan Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Nama  Usia     Hobi\n",
      "0    Ali    32    Tidur\n",
      "1   Budi    22  Ngeslot\n",
      "2  Citra    29     Judi\n",
      "3   Dina    50  ngegame\n",
      "           Usia\n",
      "count   4.00000\n",
      "mean   33.25000\n",
      "std    11.92686\n",
      "min    22.00000\n",
      "25%    27.25000\n",
      "50%    30.50000\n",
      "75%    36.50000\n",
      "max    50.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membuat DataFrame Pandas\n",
    "data_pandas = {\"Nama\": [\"Ali\", \"Budi\", \"Citra\", \"Dina\"], \"Usia\": [32, 22, 29, 50]}\n",
    "df_pandas = pd.DataFrame(data_pandas)\n",
    "\n",
    "# Membuat DataFrame kedua\n",
    "data_pandas_2 = {\"Nama\": [\"Ali\", \"Budi\", \"Citra\", \"Dina\"], \"Hobi\": [\"Tidur\", \"Ngeslot\", \"Judi\", \"ngegame\"]}\n",
    "df_pandas_2 = pd.DataFrame(data_pandas_2)\n",
    "\n",
    "# Join antara dua DataFrame\n",
    "df_joined = pd.merge(df_pandas, df_pandas_2, on=\"Nama\")\n",
    "print(df_joined)\n",
    "\n",
    "# Menghitung statistik deskriptif\n",
    "print(df_pandas.describe())\n",
    "\n",
    "# Plotting Data\n",
    "import matplotlib.pyplot as plt\n",
    "df_pandas['Usia'].plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama mengimport modul lalu membuat data frame, lalu menggabungkan dataframe dengan menggunakan pd.merge, Lalu menghitung statistik data menggunakan print dan describe, dan yang terakhir membuat plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Nama|Usia|\n",
      "+-----+----+\n",
      "|  Ali|  34|\n",
      "| Budi|  23|\n",
      "|Citra|  29|\n",
      "| Dina|  45|\n",
      "+-----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    Nama  Usia\n",
       " 0    Ali    34\n",
       " 1   Budi    23\n",
       " 2  Citra    29\n",
       " 3   Dina    45,\n",
       " None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengonversi DataFrame dari PySpark ke Pandas\n",
    "df_pandas_from_spark = df.toPandas()\n",
    "\n",
    "# Mengonversi DataFrame dari Pandas ke PySpark\n",
    "df_spark_from_pandas = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Menampilkan DataFrame hasil konversi\n",
    "df_pandas_from_spark, df_spark_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tugas 5**: Gunakan metode ini untuk menggabungkan data yang Anda buat di PySpark dengan data dari Pandas, kemudian lakukan analisis sederhana seperti menghitung rata-rata usia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (LAPTOP-CN6I63B7 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 33 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m df_spark\u001b[38;5;241m.\u001b[39munionByName(df_spark_from_pandas)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Menampilkan DataFrame hasil gabungan\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mdf_combined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Menghitung rata-rata usia\u001b[39;00m\n\u001b[0;32m     35\u001b[0m rata_rata_usia \u001b[38;5;241m=\u001b[39m df_combined\u001b[38;5;241m.\u001b[39mselect(avg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsia\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\environmentbaru\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\environmentbaru\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\environmentbaru\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\environmentbaru\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (LAPTOP-CN6I63B7 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 33 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Membuat DataFrame di pandas\n",
    "data_pandas = {\n",
    "    \"Nama\": [\"Conan\", \"bonto\", \"Narto\", \"Sadara\", \"inoksuke\"],\n",
    "    \"Usia\": [13, 12, 15, 22, 24],\n",
    "    \"Pekerjaan\": [\"samurai\", \"Pemain\", \"fakboi\", \"fakgirl\", \"atlet\"],\n",
    "    'Hobi': ['latihan', 'Bola', 'Caper', 'Caper', 'fisikan'],\n",
    "    'Gender': ['Laki-laki', 'Laki-laki', 'Perempuan', 'Perempuan', 'Perempuan']\n",
    "}\n",
    "df_pandas = pd.DataFrame(data_pandas)\n",
    "\n",
    "# Mengonversi DataFrame dari Pandas ke PySpark\n",
    "df_spark_from_pandas = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Membuat DataFrame langsung di PySpark\n",
    "data_spark = [\n",
    "    (\"frey\", 25, \"Koki\", \"Memasak\", \"Perempuan\"),\n",
    "    (\"naka\", 26, \"Pedagang\", \"Olahraga\", \"Perempuan\"),\n",
    "    (\"moto\", 35, \"Teknisi\", \"Memancing\", \"Laki-laki\")\n",
    "]\n",
    "df_spark = spark.createDataFrame(data_spark, ['Nama', 'Usia', 'Pekerjaan', 'Hobi', 'Gender'])\n",
    "\n",
    "# Menggabungkan DataFrame dari PySpark\n",
    "df_combined = df_spark.unionByName(df_spark_from_pandas)\n",
    "\n",
    "# Menampilkan DataFrame hasil gabungan\n",
    "df_combined.show()\n",
    "\n",
    "# Menghitung rata-rata usia\n",
    "rata_rata_usia = df_combined.select(avg('Usia')).collect()[0][0]\n",
    "print(f\"Rata-rata usia: {rata_rata_usia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode ini menggabungkan dua DataFrame: Satu dibuat dari Pandas dan dikonversi ke PySpark, sementara yang lainnya dibuat langsung di PySpark.\n",
    "Penggabungan dilakukan berdasarkan nama kolom: Ini memastikan bahwa data yang sesuai digabungkan dengan benar.\n",
    "Dihitung rata-rata usia: Setelah penggabungan, dihitung rata-rata usia dari semua data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Mengonversi DataFrame dari PySpark ke Pandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_pandas_from_spark \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Mengonversi DataFrame dari Pandas ke PySpark\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_spark_from_pandas \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_pandas)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Mengonversi DataFrame dari PySpark ke Pandas\n",
    "df_pandas_from_spark = df.toPandas()\n",
    "\n",
    "# Mengonversi DataFrame dari Pandas ke PySpark\n",
    "df_spark_from_pandas = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Menampilkan DataFrame hasil konversi\n",
    "df_pandas_from_spark, df_spark_from_pandas.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmentbaru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
